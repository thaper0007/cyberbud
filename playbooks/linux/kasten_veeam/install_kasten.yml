---
# ──────────────────────────────────────────────────────────────────────────────
# Play 1: Base packages + iSCSI on ALL nodes (serialized, robust apt-lock handling)
# ──────────────────────────────────────────────────────────────────────────────
- name: Ensure base packages + iSCSI on all nodes
  hosts: k3s_cluster
  become: yes
  serial: 1            # Run one host at a time to avoid apt lock races
  vars:
    base_packages:
      - curl
      - tar
      - ca-certificates
      - jq
      - nfs-common
      - open-iscsi

  pre_tasks:
    - name: Stop apt/upgrade timers (temporary)
      systemd:
        name: "{{ item }}"
        state: stopped
        masked: yes
      loop:
        - apt-daily.timer
        - apt-daily-upgrade.timer
      failed_when: false

    - name: Stop apt/upgrade services (temporary)
      systemd:
        name: "{{ item }}"
        state: stopped
      loop:
        - apt-daily.service
        - apt-daily-upgrade.service
        - unattended-upgrades.service
      failed_when: false

    - name: Stop PackageKit (can hold apt lock)
      systemd:
        name: packagekit
        state: stopped
        masked: yes
      failed_when: false

    # Wait up to ~5 minutes; if still locked, the rescue block force-cleans safely.
    - block:
        - name: Wait for apt/dpkg locks to be released
          shell: |
            set -e
            # fail if any package managers running
            for p in apt apt-get dpkg unattended-upgrade packagekitd; do
              pgrep -x "$p" >/dev/null && exit 1
            done
            # fail if any locks in use
            for f in /var/lib/dpkg/lock-frontend /var/lib/apt/lists/lock; do
              fuser "$f" >/dev/null 2>&1 && exit 1 || true
            done
          register: apt_locks_free
          retries: 60
          delay: 5
          until: apt_locks_free.rc == 0
          changed_when: false
      rescue:
        - name: Force stop lingering package managers (last resort)
          shell: |
            set -e
            systemctl stop unattended-upgrades || true
            systemctl stop packagekit || true
            pkill -9 -x apt || true
            pkill -9 -x apt-get || true
            pkill -9 -x dpkg || true
            pkill -9 -f unattended-upgrade || true
            pkill -9 -x packagekitd || true
            rm -f /var/lib/dpkg/lock-frontend /var/lib/apt/lists/lock
            dpkg --configure -a || true

  tasks:
    - name: Install base packages (incl. open-iscsi)
      apt:
        name: "{{ base_packages }}"
        state: present
        update_cache: yes
        force_apt_get: yes
        lock_timeout: 600
      environment:
        DEBIAN_FRONTEND: noninteractive

    - name: Enable and start iscsid
      systemd:
        name: iscsid
        enabled: yes
        state: started

  post_tasks:
    - name: Re-enable apt timers
      systemd:
        name: "{{ item }}"
        state: started
        masked: no
      loop:
        - apt-daily.timer
        - apt-daily-upgrade.timer
      failed_when: false

    - name: Unmask PackageKit (optional)
      systemd:
        name: packagekit
        masked: no
      failed_when: false

# ──────────────────────────────────────────────────────────────────────────────
# Play 2: K3s/Helm (if needed), Longhorn, MetalLB, and Kasten K10
# ──────────────────────────────────────────────────────────────────────────────
- name: Install K3s/Helm (if needed), Longhorn, MetalLB, and Kasten K10
  hosts: k3s_master
  become: yes

  vars:
    # General
    install_k3s_channel: "stable"
    kube_user: "parteek"
    kubeconfig_root: "/etc/rancher/k3s/k3s.yaml"
    kube_user_config: "/home/{{ kube_user }}/.kube/config"

    # Longhorn
    longhorn_ns: "longhorn-system"
    longhorn_replica_count: 1
    make_longhorn_default: true

    # MetalLB
    metallb_version_manifest: "https://raw.githubusercontent.com/metallb/metallb/v0.14.8/config/manifests/metallb-native.yaml"
    metallb_ns: "metallb-system"
    metallb_pool_range: "172.30.50.210-172.30.50.240"

    # Kasten
    kasten_ns: "kasten-io"
    kasten_cluster_name: "k8s-cluster"
    kasten_gateway_svc: "gateway"

  pre_tasks:
    # ---- K3s (server) ----
    - name: Check if k3s service exists
      shell: "systemctl list-unit-files | grep -E '^k3s\\.service'"
      register: k3s_service
      failed_when: false
      changed_when: false

    - name: Install K3s server (if missing)
      shell: "curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL={{ install_k3s_channel }} sh -s -"
      when: k3s_service.rc != 0

    - name: Ensure k3s is running and enabled
      systemd:
        name: k3s
        state: started
        enabled: yes

    - name: Wait for kubeconfig file to exist
      stat:
        path: "{{ kubeconfig_root }}"
      register: kcfg
      retries: 30
      delay: 3
      until: kcfg.stat.exists
      changed_when: false

    - name: Wait for Kubernetes API to be reachable
      command: kubectl cluster-info
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }
      register: cluster_info
      retries: 30
      delay: 5
      until: cluster_info.rc == 0

    # ---- Helm ----
    - name: Check if Helm is installed
      command: bash -lc "command -v helm"
      register: helm_check
      ignore_errors: true
      changed_when: false

    - name: Install Helm (if missing)
      shell: "curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
      args: { executable: /bin/bash }
      when: helm_check.rc != 0

    - name: Verify Helm
      command: helm version
      changed_when: false

    # ---- kubeconfig for user (handy for CLI) ----
    - name: Create user kube dir
      file:
        path: "/home/{{ kube_user }}/.kube"
        state: directory
        mode: '0755'
        owner: "{{ kube_user }}"
        group: "{{ kube_user }}"

    - name: Copy k3s kubeconfig to user
      copy:
        src: "{{ kubeconfig_root }}"
        dest: "{{ kube_user_config }}"
        owner: "{{ kube_user }}"
        group: "{{ kube_user }}"
        mode: '0600'
        remote_src: true

  tasks:
    # ================= Longhorn =================
    - name: Add Longhorn Helm repo
      command: helm repo add longhorn https://charts.longhorn.io
      register: lh_repo
      changed_when: "'has been added' in (lh_repo.stdout + lh_repo.stderr) or 'already exists' in (lh_repo.stdout + lh_repo.stderr)"

    - name: Helm repo update
      command: helm repo update

    - name: Install/upgrade Longhorn
      command: >
        helm upgrade --install longhorn longhorn/longhorn
        -n {{ longhorn_ns }} --create-namespace
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    - name: Wait for Longhorn manager to be ready
      command: "kubectl -n {{ longhorn_ns }} rollout status deploy/longhorn-manager --timeout=300s"
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    - name: Set Longhorn default replica count (lab)
      command: >
        kubectl -n {{ longhorn_ns }} patch settings.longhorn.io default-replica-count
        --type=merge -p '{{ {"value": longhorn_replica_count|string} | to_json }}'
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    - name: Make Longhorn default StorageClass (optional)
      when: make_longhorn_default
      command: >
        kubectl patch storageclass longhorn
        -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }
      register: sc_patch
      failed_when: false

    # ================= MetalLB =================
    - name: Install MetalLB components
      command: "kubectl apply -f {{ metallb_version_manifest }}"
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    - name: Wait for MetalLB controller Ready
      command: "kubectl -n {{ metallb_ns }} rollout status deploy/controller --timeout=180s"
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    - name: Wait for MetalLB speaker Ready
      command: "kubectl -n {{ metallb_ns }} rollout status ds/speaker --timeout=180s"
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    - name: Apply MetalLB IPAddressPool
      copy:
        dest: /tmp/metallb-ip-pool.yaml
        content: |
          apiVersion: metallb.io/v1beta1
          kind: IPAddressPool
          metadata:
            name: lb-pool
            namespace: {{ metallb_ns }}
          spec:
            addresses:
              - {{ metallb_pool_range }}

    - name: kubectl apply IPAddressPool
      command: kubectl apply -f /tmp/metallb-ip-pool.yaml
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    - name: Apply MetalLB L2Advertisement
      copy:
        dest: /tmp/metallb-l2adv.yaml
        content: |
          apiVersion: metallb.io/v1beta1
          kind: L2Advertisement
          metadata:
            name: lb-advert
            namespace: {{ metallb_ns }}
          spec:
            ipAddressPools:
              - lb-pool

    - name: kubectl apply L2Advertisement
      command: kubectl apply -f /tmp/metallb-l2adv.yaml
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    # ================= Kasten K10 =================
    - name: Add Kasten Helm repo
      command: helm repo add kasten https://charts.kasten.io/
      register: k_repo
      changed_when: "'has been added' in (k_repo.stdout + k_repo.stderr) or 'already exists' in (k_repo.stdout + k_repo.stderr)"

    - name: Helm repo update (again)
      command: helm repo update

    - name: Install/upgrade Kasten K10
      command: >
        helm upgrade --install k10 kasten/k10
        --namespace {{ kasten_ns }} --create-namespace
        --set global.clusterName="{{ kasten_cluster_name }}"
        --set global.persistence.storageClass=longhorn
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }

    - name: Expose K10 gateway via LoadBalancer
      command: >
        kubectl -n {{ kasten_ns }} patch svc {{ kasten_gateway_svc }}
        -p '{"spec":{"type":"LoadBalancer"}}'
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }
      failed_when: false

    # PVC & pending pods auto-recovery
    - name: Wait for Longhorn CSI driver to be registered
      command: bash -lc "kubectl get csidrivers | grep -q driver.longhorn.io"
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }
      register: csi_ok
      retries: 30
      delay: 5
      until: csi_ok.rc == 0

    - name: Restart Kasten deployments that need PVCs
      command: kubectl -n {{ kasten_ns }} rollout restart deploy/{{ item }}
      loop: [ "catalog-svc", "jobs-svc", "logging-svc", "metering-svc", "prometheus-server" ]
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }
      failed_when: false

    - name: Wait for those deployments to become Ready
      command: kubectl -n {{ kasten_ns }} rollout status deploy/{{ item }} --timeout=300s
      loop: [ "catalog-svc", "jobs-svc", "logging-svc", "metering-svc", "prometheus-server" ]
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }
      failed_when: false

    - name: Wait for K10 gateway external IP from MetalLB
      command: >
        bash -lc "kubectl -n {{ kasten_ns }} get svc {{ kasten_gateway_svc }}
        -o jsonpath='{.status.loadBalancer.ingress[0].ip}'"
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }
      register: gw_ip
      retries: 30
      delay: 5
      until: gw_ip.stdout | length > 0

    - name: Final status - pods/PVCs/service
      shell: |
        echo "--- Pods ---"
        kubectl -n {{ kasten_ns }} get pods -o wide
        echo "--- PVCs ---"
        kubectl -n {{ kasten_ns }} get pvc
        echo "--- Service ---"
        kubectl -n {{ kasten_ns }} get svc {{ kasten_gateway_svc }} -o wide
      environment: { KUBECONFIG: "{{ kubeconfig_root }}" }
      register: final_status
      changed_when: false

    - debug:
        msg:
          - "{{ final_status.stdout }}"
          - "Kasten UI: http://{{ gw_ip.stdout }}/k10/"
